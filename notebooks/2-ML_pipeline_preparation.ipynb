{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation & Algorithm choice\n",
    "This notebook will be used to explore some possibilities about feature engineering and model evaluation + choice.  \n",
    "We are dealing with text and we want to classify some input text into one to many categories. \n",
    "\n",
    "**We will build a supervised NLP model with multilabel classification (and not multiclass!).**  \n",
    "What is the difference? \n",
    "Well, _\"in multi-class problems the classes are mutually exclusive, whereas for multi-label problems each label represents a different classification task\"_ [source](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "\n",
    "# NLP transformations\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# sklearn pipelines import\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "# sklearn estimators and transformers\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Multiclass\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import classification_report, f1_score, make_scorer, precision_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load data from database on disk and build X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/output/disaster.db')\n",
    "df = pd.read_sql_table('messages', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message   genre  related  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...  direct        1   \n",
       "1   7            Is the Hurricane over or is it not over  direct        1   \n",
       "2   8                    Looking for someone but no name  direct        1   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4  12  says: west side of Haiti, rest of the country ...  direct        1   \n",
       "\n",
       "   request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        0      0            0             0                 0   \n",
       "1        0      0            1             0                 0   \n",
       "2        0      0            0             0                 0   \n",
       "3        1      0            1             0                 1   \n",
       "4        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  ...  aid_centers  other_infrastructure  weather_related  \\\n",
       "0                  0  ...            0                     0                0   \n",
       "1                  0  ...            0                     0                1   \n",
       "2                  0  ...            0                     0                0   \n",
       "3                  0  ...            0                     0                0   \n",
       "4                  0  ...            0                     0                0   \n",
       "\n",
       "   floods  storm  fire  earthquake  cold  other_weather  direct_report  \n",
       "0       0      0     0           0     0              0              0  \n",
       "1       0      1     0           0     0              0              0  \n",
       "2       0      0     0           0     0              0              0  \n",
       "3       0      0     0           0     0              0              0  \n",
       "4       0      0     0           0     0              0              0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* X (input feature columns) is just the `message` (text), the `id` will not help us and the `genre` neither\n",
    "* target classes are all other columns (36 categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['message']\n",
    "Y = df.drop(['id', 'message', 'genre'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Weather update - a cold front from Cuba that c...\n",
       "1              Is the Hurricane over or is it not over\n",
       "2                      Looking for someone but no name\n",
       "3    UN reports Leogane 80-90 destroyed. Only Hospi...\n",
       "4    says: west side of Haiti, rest of the country ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>search_and_rescue</th>\n",
       "      <th>security</th>\n",
       "      <th>military</th>\n",
       "      <th>water</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   related  request  offer  aid_related  medical_help  medical_products  \\\n",
       "0        1        0      0            0             0                 0   \n",
       "1        1        0      0            1             0                 0   \n",
       "2        1        0      0            0             0                 0   \n",
       "3        1        1      0            1             0                 1   \n",
       "4        1        0      0            0             0                 0   \n",
       "\n",
       "   search_and_rescue  security  military  water  ...  aid_centers  \\\n",
       "0                  0         0         0      0  ...            0   \n",
       "1                  0         0         0      0  ...            0   \n",
       "2                  0         0         0      0  ...            0   \n",
       "3                  0         0         0      0  ...            0   \n",
       "4                  0         0         0      0  ...            0   \n",
       "\n",
       "   other_infrastructure  weather_related  floods  storm  fire  earthquake  \\\n",
       "0                     0                0       0      0     0           0   \n",
       "1                     0                1       0      1     0           0   \n",
       "2                     0                0       0      0     0           0   \n",
       "3                     0                0       0      0     0           0   \n",
       "4                     0                0       0      0     0           0   \n",
       "\n",
       "   cold  other_weather  direct_report  \n",
       "0     0              0              0  \n",
       "1     0              0              0  \n",
       "2     0              0              0  \n",
       "3     0              0              0  \n",
       "4     0              0              0  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. NLP tasks\n",
    "In the project they suggest us to use **[nltk](https://www.nltk.org/)** which is a famous NLP library. I will instead use another package which has also became very famous recently: **[spacy](https://spacy.io/)** because I find this package easier to use.  \n",
    "\n",
    "Before going further, you need to ensure that you have spacy package installed with the _'en_core_web_sm'_ downloaded. This can be done with:\n",
    "> ```pip install spacy```  \n",
    "> ```python -m spacy download en_core_web_sm```\n",
    "\n",
    "First step is to load the downloaded module. I have obviously chosen one in english and a small one (note the trailing _'sm'_ in the _'en_core_web_sm'_ name). There are larger but spacy recommends us to always start with small and go to larger only if needed.  \n",
    "**Note:** because I will not use it, it is a good option to provide a list of submodules that we do not want spacy to load so that the operation is faster. Please refer to this [spacy documentation about pipeline](https://spacy.io/usage/processing-pipelines) to discover all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency labels, Name Entity Recognition and Text Categorization are not need for our specific usage\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner', 'textcat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the text transformation, there are easy tricks such as putting everything in lower case, remove digits and stop words, etc. Then we have the [choice between Stemming or Lemmatization](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python):  \n",
    "_\"Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words.\"_  \n",
    "* \"Stemming is different to Lemmatization in the approach it uses to produce root forms of words and the word produced. Stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\"\n",
    "* \"Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma is the canonical form, dictionary form, or citation form of a set of words.\"\n",
    "\n",
    "I will **go for Lemmatization** and for that I will just use the spacy NLP module that has been loaded and remove punctuation, whitespaces, stop words, digits, url and emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_txt(x):\n",
    "    \"\"\"\n",
    "    Lemmatize text from a string.\n",
    "    :param x: (string) the text we want to lemmatize\n",
    "    :return: (string) string containing the lemmatized text.\n",
    "    \"\"\"\n",
    "    doc = nlp.tokenizer(x.lower())\n",
    "    lemma_txt = [token.lemma_ for token in doc if not \n",
    "              token.is_punct |\n",
    "              token.is_space |\n",
    "              token.is_stop |\n",
    "              token.is_digit |\n",
    "              token.is_quote |\n",
    "              token.is_bracket |\n",
    "              token.is_currency |\n",
    "              token.like_url |\n",
    "              token.like_email ]\n",
    "    lemma_txt = ' '.join(lemma_txt)\n",
    "    return lemma_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'be', 'talk', 'say']\n"
     ]
    }
   ],
   "source": [
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sebastian thrun start work self drive car google people outside company take seriously tell senior ceos major american car company shake hand turn away not worth talk say thrun interview recode early week'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize_txt(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Build pipelines models\n",
    "### 3.1. Models to try\n",
    "Next phase is to build some different pipelines, one for each model that I will try:\n",
    "* [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), simple and basic model when dealing with classification\n",
    "* [RandomForest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), a tree based algorithm that I wand to try because it is an ensemble model but easier and faster to use than [GradientBoostingTree](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) or even [XGBoost](https://xgboost.readthedocs.io/en/latest/) that can both take some huge computation time.\n",
    "\n",
    "### 3.2. How to perform multiple class classification?\n",
    "* As per scikit-learn documentation, for Logistic Regression: _\"In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme\"_. So the Logistic Regression classifier will be wrapped by a [OneVsRestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) which consists in _\"fitting one classifier per class. For each classifier, the class is fitted against all the other classes\"_.\n",
    "* For the RandomForest we can wrapper it within a [MultiOutputClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) which can be used to _\"extend classifiers that do not natively support multi-target classification\"_.\n",
    "\n",
    "There is a sample of the usage of MultiOutputClassifier with RandomForestClassifier on [scikit-learn documentation about multiclass](https://scikit-learn.org/stable/modules/multiclass.html).\n",
    "\n",
    "### 3.3. For the pipeline, use _FeatureUnion_ or  _ColumnTransformer_ ?\n",
    "Based on this [reading](https://stackoverflow.com/questions/55604249/featureunion-vs-columntransformer) I have decided to go for [FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) usage because it is said that:  \n",
    "_\"FeatureUnion applies different transformers to the whole of the input data and then combines the results by concatenating them.  \n",
    "ColumnTransformer, on the other hand, applies different transformers to different subsets of the whole input data, and again concatenates the results.\"_\n",
    "\n",
    "In our case, there is only one feature which is the original text message on which we want to apply several transformations.\n",
    "\n",
    "#### 3.3.1. TF-IDF\n",
    "Let's start with a very common feature engineering in NLP: TF-IDF vectorization. This will allow us to give more weight to important words (based on their frequency in a text and the number of documents/texts they appear in). The parameters are the ones from the sklearn [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), they are all eligible to tuning through GridSearch but for the first run let's put some arbitrary values:\n",
    "* max_df: 0.5 to ignore terms that have a document frequency strictly higher than this threshold (50%)\n",
    "* max_features: 5000 seems to be a pretty good choice to start with. It will help us to build a vocabulary that only consider the top 5000 ordered by term frequency across the corpus.\n",
    "* ngram_range: here we specify that we also want to consider bi-grams\n",
    "* binary: if set to True we force all non-zero term counts to be set to 1 (even if they appear more than that)\n",
    "* norm: no regularization for the moment (default is 'l2')\n",
    "* use_idf: do we have to use the Inverse Document Frequency reweighting? (default is True)\n",
    "\n",
    "#### 3.3.2. Logistic Regression with OneVsRest pipeline\n",
    "As per sklearn documentation, solver to use is liblinear which \"is limited to one-versus-rest schemes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline([\n",
    "            (\"features\", FeatureUnion([\n",
    "                (\"text\", TfidfVectorizer(tokenizer=lemmatize_txt, max_df=0.5, \n",
    "                                 max_features=5000, ngram_range=(1, 2),\n",
    "                                 binary=False, norm=None, \n",
    "                                 use_idf=True))\n",
    "            ])),    \n",
    "            (\"clf\", OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3. RandomForest with MultiOutputClassifier pipeline\n",
    "Here the RandomForest classifier has few parameters set and no tuning made:\n",
    "* random state to ensure reproducibility\n",
    "* an arbitrary number of estimators\n",
    "* max_depth also to limit the tree size and computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = Pipeline([\n",
    "            (\"features\", FeatureUnion([\n",
    "                (\"text\", TfidfVectorizer(tokenizer=lemmatize_txt, max_df=0.5, \n",
    "                                 max_features=5000, ngram_range=(1, 2),\n",
    "                                 binary=False, norm=None, \n",
    "                                 use_idf=True))\n",
    "            ])),    \n",
    "            (\"clf\", MultiOutputClassifier(RandomForestClassifier(random_state=42, max_depth=8,\n",
    "                                                                 n_estimators=300)))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Split into train/test and build models pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_val, x_test, y_train_val, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=5000, min_df=1,\n",
       "        ngr...ne, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None))])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_lr.fit(x_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=5000, min_df=1,\n",
       "        ngr...           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_rf.fit(x_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## 5. Models evaluation\n",
    "How can we evaluate? As explained in this [towardsdatascience post](https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff) we have to average our results made for all categories and for that there are 2 options: the micro-averaging way or the macro-averaging one.  \n",
    "Read this post for further information if needed, I will go for the micro way as \"it is a useful measure when your dataset varies in size\" (and remember that our classes/targets are imbalanced).  \n",
    "Another interesting source about the difference and what to choose is available [here](https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin) where it is written that **_\"In a multi-class classification setup, micro-average is preferable if you suspect there might be class imbalance (i.e you may have many more examples of one class than of other classes).\"_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_perf(model_name, pipeline, x_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model performance by predicting on test dataset and then printing performance metrics\n",
    "    :param model_name: (string) name of the model, just for display purpose\n",
    "    :pipeline: (object) the sklearn classifier pipeline to use for inference\n",
    "    :x_test: (pandas DataFrame) features of the test subset used to evaluate\n",
    "    :y_test: (pandas DataFrame) ground truth targets for the test subset used to evaluate\n",
    "    \"\"\"\n",
    "    y_pred = pipeline.predict(x_test)\n",
    "    \n",
    "    p = precision_score(y_test, y_pred, average=\"micro\")\n",
    "    r = recall_score(y_test, y_pred, average=\"micro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"micro\")\n",
    "    wp = precision_score(y_test, y_pred, average=\"weighted\")\n",
    "    wr = recall_score(y_test, y_pred, average=\"weighted\")\n",
    "    wf1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    \n",
    "    print(\"Model {} -- MICRO metrics -- Precision: {:.4f}, Recall: {:.4f}, F1-Score: {:.4f}\".format(model_name, p, r, f1))\n",
    "    print(\"Model {} -- WEIGHTED metrics -- Precision: {:.4f}, Recall: {:.4f}, F1-Score: {:.4f}\".format(model_name, wp, wr, wf1))\n",
    "    \n",
    "    # Build a df with results, class per class\n",
    "    records = []\n",
    "    records.append(('GLOBAL', f1))\n",
    "    for i, col in enumerate(y_test.columns):\n",
    "        records.append((col, f1_score(y_test[col], y_pred.T[i])))\n",
    "    \n",
    "    df = pd.DataFrame.from_records(records, columns=['class', 'F1-Score {}'.format(model_name)])\n",
    "    df.set_index('class')\n",
    "    return y_pred, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Results for our first models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticReg -- MICRO metrics -- Precision: 0.6726, Recall: 0.5169, F1-Score: 0.5845\n",
      "Model LogisticReg -- WEIGHTED metrics -- Precision: 0.6228, Recall: 0.5169, F1-Score: 0.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RandomForest -- MICRO metrics -- Precision: 0.7683, Recall: 0.3281, F1-Score: 0.4599\n",
      "Model RandomForest -- WEIGHTED metrics -- Precision: 0.6506, Recall: 0.3281, F1-Score: 0.3327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr, df_lr = evaluate_model_perf('LogisticReg', pipeline_lr, x_test, y_test)\n",
    "y_pred_rf, df_rf = evaluate_model_perf('RandomForest', pipeline_rf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1-Score LogisticReg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>GLOBAL</td>\n",
       "      <td>0.584541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>related</td>\n",
       "      <td>0.862293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>request</td>\n",
       "      <td>0.519973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>offer</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aid_related</td>\n",
       "      <td>0.624843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>medical_help</td>\n",
       "      <td>0.311111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>medical_products</td>\n",
       "      <td>0.325123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>search_and_rescue</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>security</td>\n",
       "      <td>0.017699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>military</td>\n",
       "      <td>0.248996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>water</td>\n",
       "      <td>0.519164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>food</td>\n",
       "      <td>0.623106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>shelter</td>\n",
       "      <td>0.455959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>clothing</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>money</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>missing_people</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>refugees</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>death</td>\n",
       "      <td>0.308483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>other_aid</td>\n",
       "      <td>0.184946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>infrastructure_related</td>\n",
       "      <td>0.147186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>transport</td>\n",
       "      <td>0.173653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>buildings</td>\n",
       "      <td>0.364964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>electricity</td>\n",
       "      <td>0.309392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>tools</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>hospitals</td>\n",
       "      <td>0.112360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>shops</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>aid_centers</td>\n",
       "      <td>0.056075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>other_infrastructure</td>\n",
       "      <td>0.091873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>weather_related</td>\n",
       "      <td>0.651236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>floods</td>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>storm</td>\n",
       "      <td>0.465786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fire</td>\n",
       "      <td>0.217391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>earthquake</td>\n",
       "      <td>0.762322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cold</td>\n",
       "      <td>0.316940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>other_weather</td>\n",
       "      <td>0.138298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>direct_report</td>\n",
       "      <td>0.435214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        F1-Score LogisticReg\n",
       "class                                       \n",
       "GLOBAL                              0.584541\n",
       "related                             0.862293\n",
       "request                             0.519973\n",
       "offer                               0.034483\n",
       "aid_related                         0.624843\n",
       "medical_help                        0.311111\n",
       "medical_products                    0.325123\n",
       "search_and_rescue                   0.220000\n",
       "security                            0.017699\n",
       "military                            0.248996\n",
       "water                               0.519164\n",
       "food                                0.623106\n",
       "shelter                             0.455959\n",
       "clothing                            0.344828\n",
       "money                               0.285714\n",
       "missing_people                      0.043478\n",
       "refugees                            0.240000\n",
       "death                               0.308483\n",
       "other_aid                           0.184946\n",
       "infrastructure_related              0.147186\n",
       "transport                           0.173653\n",
       "buildings                           0.364964\n",
       "electricity                         0.309392\n",
       "tools                               0.066667\n",
       "hospitals                           0.112360\n",
       "shops                               0.000000\n",
       "aid_centers                         0.056075\n",
       "other_infrastructure                0.091873\n",
       "weather_related                     0.651236\n",
       "floods                              0.568182\n",
       "storm                               0.465786\n",
       "fire                                0.217391\n",
       "earthquake                          0.762322\n",
       "cold                                0.316940\n",
       "other_weather                       0.138298\n",
       "direct_report                       0.435214"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5.2. Let's have a better understanding of micro, macro and weighted average\n",
    "Let's try to analyze for one class, the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.39      0.47      1306\n",
      "           1       0.82      0.91      0.86      3938\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      5244\n",
      "   macro avg       0.71      0.65      0.67      5244\n",
      "weighted avg       0.76      0.78      0.76      5244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test['related'], y_pred_lr.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 510,  796],\n",
       "       [ 350, 3588]], dtype=int64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We will need the confusion matrix to see how many elements were classified as positive in this category\n",
    "confusion_matrix(y_test['related'], y_pred_lr.T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 1 is when truth is 0 whereas column 1 is when prediction is 0. So we have here:\n",
    "* 510 True Negative (TN) (truth is O and prediction is 0 as well)\n",
    "* 3588 True Positive (TP) (same for the 1)\n",
    "* 796 False Positive (FP) (we say it is positive whereas actually it is not)\n",
    "* 350 False Negative (FN) (we say it is negative and actually it is positive)\n",
    "\n",
    "Precision formula is : TP/(TP + FP) and Recall one is TP/(TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Positive is 0.82, Recall is 0.91\n",
      "Precision for Negative is 0.59, Recall is 0.39\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision for Positive is {:.2f}, Recall is {:.2f}\".format(3588/(3588+796), 3588/(3588+350)))\n",
    "print(\"Precision for Negative is {:.2f}, Recall is {:.2f}\".format(510/(510+350), 510/(510+796)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 1306 samples that should be classified as Negative and 3938 as Positive (cf. 'support' column).  \n",
    "The macro average is a classic mean of both Precision for Positive and Precision for Negative.  \n",
    "The weighted one takes into account the ratio 1306/(1306+3938) which is around 25%.\n",
    "The micro one is still a mystery..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision macro is 0.705, Precision weighted is 0.763\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision macro is {}, Precision weighted is {:.3f}\".format((0.59+0.82)/2, (1306*0.59+3938*0.82)/(1306+3938)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For micro averaging it is a little bit tricky but the goal is to sum all true positives and divide by the sum of all true positives plus the sum of all false positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro averaged precision is 0.78\n"
     ]
    }
   ],
   "source": [
    "print(\"Micro averaged precision is {:.2f}\".format((3588+510)/(1306+3938)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be the same value for recall as we will still have the same amount of True Positive (3597+510) and now we take the False Negative. But, as very well explained in this [great post](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1) with pictures, _\"each prediction error (X is misclassified as Y) is a False Positive for Y, and a False Negative for X. Thus, the total number of False Negatives is again the total number of prediction errors\"_\n",
    "\n",
    "When looking at a specific class in a multilabel classification, taking micro average metric for a specific class has no sense but on the overall this is a much better metric than the macro one that will just perform a mean operation over all precision, recall and F1 metrics. The weighted average is also a good option as it is taking into account the number of samples, that's why **I am looking at both of them, micro and weighted**.\n",
    "\n",
    "***Observations about the first results:***\n",
    "* The **Logistic Regression has a best F1-Score than the Random Forest (so far)** (0.58 vs. 0.46).\n",
    "* When using the `weighted` average method (instead of `micro`) then the result is even lower for both of them.\n",
    "* There are classes with a F1-Score very low, near 0 (for instance `offer` or `security`)\n",
    "\n",
    "Let's print the full classification report so that we will also have access to the number of samples for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.91      0.86      3938\n",
      "           1       0.66      0.43      0.52       895\n",
      "           2       0.03      0.04      0.03        26\n",
      "           3       0.67      0.58      0.62      2131\n",
      "           4       0.47      0.23      0.31       422\n",
      "           5       0.49      0.24      0.33       270\n",
      "           6       0.30      0.17      0.22       127\n",
      "           7       0.04      0.01      0.02        88\n",
      "           8       0.33      0.20      0.25       155\n",
      "           9       0.63      0.44      0.52       339\n",
      "          10       0.71      0.55      0.62       595\n",
      "          11       0.58      0.37      0.46       470\n",
      "          12       0.47      0.27      0.34        73\n",
      "          13       0.33      0.25      0.29       104\n",
      "          14       0.06      0.03      0.04        60\n",
      "          15       0.32      0.19      0.24       171\n",
      "          16       0.39      0.25      0.31       237\n",
      "          17       0.37      0.12      0.18       695\n",
      "          18       0.25      0.10      0.15       328\n",
      "          19       0.31      0.12      0.17       240\n",
      "          20       0.52      0.28      0.36       267\n",
      "          21       0.47      0.23      0.31       122\n",
      "          22       0.07      0.06      0.07        32\n",
      "          23       0.12      0.11      0.11        46\n",
      "          24       0.00      0.00      0.00        22\n",
      "          25       0.07      0.04      0.06        67\n",
      "          26       0.22      0.06      0.09       223\n",
      "          27       0.75      0.58      0.65      1438\n",
      "          28       0.68      0.49      0.57       411\n",
      "          29       0.56      0.40      0.47       486\n",
      "          30       0.26      0.19      0.22        53\n",
      "          31       0.80      0.73      0.76       478\n",
      "          32       0.44      0.25      0.32       117\n",
      "          33       0.26      0.09      0.14       276\n",
      "          34       0.59      0.34      0.44      1021\n",
      "\n",
      "   micro avg       0.67      0.52      0.58     16423\n",
      "   macro avg       0.40      0.27      0.32     16423\n",
      "weighted avg       0.62      0.52      0.55     16423\n",
      " samples avg       0.57      0.45      0.45     16423\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Specific error analysis for categories `offer` and `security`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99      5218\n",
      "           1       0.03      0.04      0.03        26\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      5244\n",
      "   macro avg       0.51      0.52      0.51      5244\n",
      "weighted avg       0.99      0.99      0.99      5244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test['offer'], y_pred_lr.T[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      5156\n",
      "           1       0.04      0.01      0.02        88\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      5244\n",
      "   macro avg       0.51      0.50      0.50      5244\n",
      "weighted avg       0.97      0.98      0.97      5244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test['security'], y_pred_lr.T[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5187,   31],\n",
       "       [  25,    1]], dtype=int64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test['offer'], y_pred_lr.T[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5132,   24],\n",
       "       [  87,    1]], dtype=int64)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test['security'], y_pred_lr.T[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations:*** there are **too few samples that should be classified as 1 so our model tends to classify them into 0** and is still good when looking at the micro average value. Even the weighted one is good.  \n",
    "Question: how did the model learn for those categories? Let's count how many samples for each of those 2 categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 samples with class security in training dataset (so 1.83% of the dataset)\n"
     ]
    }
   ],
   "source": [
    "print(\"{} samples with class {} in training dataset (so {:.2f}% of the dataset)\".\n",
    "      format(y_train_val['security'].sum(), 'security', y_train_val['security'].sum()*100/len(y_train_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 samples with class security in training dataset (so 0.44% of the dataset)\n"
     ]
    }
   ],
   "source": [
    "print(\"{} samples with class {} in training dataset (so {:.2f}% of the dataset)\".\n",
    "      format(y_train_val['offer'].sum(), 'security', y_train_val['offer'].sum()*100/len(y_train_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Adding new features to the pipelines\n",
    "Before trying to fine tune those models to improve the metrics, we could try to add some features as the only one we got for the moment is the TF-IDF vectorization. Reading this [post on Kaggle](https://www.kaggle.com/shaz13/feature-engineering-for-nlp-classification), we could add some other informations to see if it helps such as:\n",
    "* number of sentences, words\n",
    "* length of the text, length of sentences\n",
    "* by using the part-of-speech tags (a.k.a POS), count the number of NOUNS, VERBS, ADJECTIVES and so on\n",
    "* ...\n",
    "\n",
    "Okay, once this is said, how do we do that? Thanks to sickit-learn we can build our own Transformers instances and put them in the pipeline.\n",
    "\n",
    "### 6.1. Looking for POS tags with Spacy\n",
    "As per [Spacy documenation on POS tags](https://spacy.io/api/annotation#pos-tagging), we will look after:\n",
    "* adjectives (POS='ADJ')\n",
    "* nouns (POS='NOUN')\n",
    "* verbs (POS='VERB')  \n",
    "Particles, pronouns, determiners, auxiliaries and so on are less relevant and should not bring very useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos(text, pos):\n",
    "    doc = nlp(text)\n",
    "    elements = [token for token in doc if token.pos_ == pos]\n",
    "    return elements, len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* How do beneficiaries (gender/economic status) in varying circumstances (emergency/non-emergency) spend cash?\n",
      "Verbs: ([do, varying, spend], 3)\n",
      "Nouns: ([beneficiaries, gender, status, circumstances, emergency, emergency, cash], 7)\n",
      "Adjectives: ([economic, non], 2)\n"
     ]
    }
   ],
   "source": [
    "text = x_train_val.loc[22944]\n",
    "print(text)\n",
    "print(\"Verbs:\", get_pos(text, \"VERB\"))\n",
    "print(\"Nouns:\", get_pos(text, \"NOUN\"))\n",
    "print(\"Adjectives:\", get_pos(text, \"ADJ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Build custom transformers\n",
    "The next section has been developped with the help of this [blog post](https://michelleful.github.io/code-blog/2015/06/20/pipelines/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCountExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe and outputs number of words\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def nb_words(self, text):\n",
    "        return len(text.split())\n",
    "\n",
    "    def transform(self, x):\n",
    "        \"\"\"\n",
    "        Need to encapsulate the result within a DataFrame otherwise an error is thrown (\"ValueError: blocks[0,:] has \n",
    "        incompatible row dimensions. Got blocks[0,1].shape[0] == 1, expected 20972.\")\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(x.apply(self.nb_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageWordLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe and outputs average word length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def average_word_length(self, text):\n",
    "        \"\"\"\n",
    "        Sometimes text is empty so need to handle this case\n",
    "        \"\"\"\n",
    "        return np.mean([len(word) for word in text.split()]) if len(text.split()) > 0 else 0\n",
    "\n",
    "    def transform(self, x):\n",
    "        return pd.DataFrame(x.apply(self.average_word_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    \"\"\"\n",
    "    Homemade function to retrieve all sentences within a given text. I have decided that sentences have \n",
    "    words and several characters and should end by either a '.', a '?' or a '!'\n",
    "    \"\"\"\n",
    "    # Add a trailing dot if the last character is not \".\", \"?\" or \"!\" to ensure we will capture the last sentence\n",
    "    if text.strip()[-1:] not in ['.', '?', '!']:\n",
    "        text = text + '.'\n",
    "    sentence_detection_regex = r\"\\s?[\\w,-;:'()\\\"\\s]+[.?!]\"\n",
    "    sentences = re.findall(sentence_detection_regex, text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceCountExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe and outputs number of sentences\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def nb_sentences(self, text):\n",
    "        return len(get_sentences(text))\n",
    "\n",
    "    def transform(self, x):\n",
    "        return pd.DataFrame(x.apply(self.nb_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageSentenceLengthExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe and outputs average sentence length\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def average_sentence_length(self, text):\n",
    "        return np.mean([len(sentence) for sentence in get_sentences(text)]) if len(get_sentences(text)) > 0 else 0\n",
    "\n",
    "    def transform(self, x):\n",
    "        return pd.DataFrame(x.apply(self.average_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(doc, pos):\n",
    "    return len([token for token in doc if token.pos_ == pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosCountExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Takes in dataframe and outputs number of words tagged as the given part-of-speech (POS)\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def nb_pos(self, text):\n",
    "        \"\"\"\n",
    "        For performance reasons, we load through spacy nlp only once and get all counts we are interested in\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        nb_nouns = count_pos(doc, \"NOUN\")\n",
    "        nb_verbs = count_pos(doc, \"VERB\")\n",
    "        nb_adjectives = count_pos(doc, \"ADJ\")\n",
    "        return nb_nouns, nb_verbs, nb_adjectives\n",
    "\n",
    "    def transform(self, x):\n",
    "        df = pd.DataFrame(x.apply(self.nb_pos))\n",
    "        # At this point all we have is a dataframe with only one column where each value is a tuple (nb_nouns, nb_verbs, nb_adjectives)\n",
    "        # We have to split and transform into 3 columns\n",
    "        df = df.astype(str).message.str[1:-1].str.split(',', expand=True)\n",
    "        # Do not forget to put back all values as numeric ones\n",
    "        return df.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Putting it all together in a new pipeline\n",
    "#### 6.3.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_lr_v2 = Pipeline([\n",
    "            (\"features\", FeatureUnion([\n",
    "                (\"text\", TfidfVectorizer(tokenizer=lemmatize_txt, max_df=0.5, \n",
    "                                 max_features=5000, ngram_range=(1, 2),\n",
    "                                 binary=False, norm=None, \n",
    "                                 use_idf=False)),\n",
    "                (\"word-count\", WordCountExtractor()),\n",
    "                (\"word-len\", AverageWordLengthExtractor()),\n",
    "                (\"sentence-count\", SentenceCountExtractor()),\n",
    "                (\"sentence-len\", AverageSentenceLengthExtractor()),\n",
    "                (\"verb-count\", PosCountExtractor())\n",
    "            ])),    \n",
    "            (\"clf\", OneVsRestClassifier(LogisticRegression(solver='liblinear')))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=5000, min_df=1,\n",
       "        ngr...ne, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None))])"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_lr_v2.fit(x_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticReg -- MICRO metrics -- Precision: 0.7335, Recall: 0.5163, F1-Score: 0.6061\n",
      "Model LogisticReg -- WEIGHTED metrics -- Precision: 0.6798, Recall: 0.5163, F1-Score: 0.5639\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr_v2, _ = evaluate_model_perf('LogisticReg', pipeline_lr_v2, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations:*** we have made progress on F1-Score metric.  \n",
    "Note that the fit on training dataset took almost 3 times the time it took with just the TF-IDF vectorization. This might be an issue to deal with as it will take ages if we want to run GridSearch Cross Validation (around 15 minutes for each combination).\n",
    "\n",
    "As a reminder, previous results were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model LogisticReg -- MICRO metrics -- Precision: 0.6726, Recall: 0.5169, F1-Score: 0.5845\n",
      "Model LogisticReg -- WEIGHTED metrics -- Precision: 0.6228, Recall: 0.5169, F1-Score: 0.5535\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr, _ = evaluate_model_perf('LogisticReg', pipeline_lr, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So **we have 2% more on F1-Score and this is mostly due to a big increase on _precision_** (the _recall_ remained more or less the same).\n",
    "\n",
    "#### 6.3.2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf_v2 = Pipeline([\n",
    "            (\"features\", FeatureUnion([\n",
    "                (\"text\", TfidfVectorizer(tokenizer=lemmatize_txt, max_df=0.5, \n",
    "                                 max_features=5000, ngram_range=(1, 2),\n",
    "                                 binary=False, norm=None, \n",
    "                                 use_idf=False)),\n",
    "                (\"word-count\", WordCountExtractor()),\n",
    "                (\"word-len\", AverageWordLengthExtractor()),\n",
    "                (\"sentence-count\", SentenceCountExtractor()),\n",
    "                (\"sentence-len\", AverageSentenceLengthExtractor()),\n",
    "                (\"verb-count\", PosCountExtractor())\n",
    "            ])),    \n",
    "            (\"clf\", MultiOutputClassifier(RandomForestClassifier(random_state=42, max_depth=8,\n",
    "                                                                 n_estimators=300)))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=5000, min_df=1,\n",
       "        ngr...           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "           n_jobs=None))])"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipeline_rf_v2.fit(x_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RandomForest -- MICRO metrics -- Precision: 0.7656, Recall: 0.3250, F1-Score: 0.4563\n",
      "Model RandomForest -- WEIGHTED metrics -- Precision: 0.6381, Recall: 0.3250, F1-Score: 0.3305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf_v2, df_rf_v2 = evaluate_model_perf('RandomForest', pipeline_rf_v2, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model RandomForest -- MICRO metrics -- Precision: 0.7683, Recall: 0.3281, F1-Score: 0.4599\n",
      "Model RandomForest -- WEIGHTED metrics -- Precision: 0.6506, Recall: 0.3281, F1-Score: 0.3327\n"
     ]
    }
   ],
   "source": [
    "y_pred_rf, df_rf = evaluate_model_perf('RandomForest', pipeline_rf, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** with the random forest and arbitrary chosen parameters, the new features did not help, the F1-Score is even slightly lower.\n",
    "\n",
    "For the tuning phase, as the Logistic Regression has less parameters than the Random Forest and is less longer to train, I will first fine tune the TF-IDF with Logistic Regression. Then, I will take the best parameters and apply them to Random Forest and perform another GridSearch to see if I am able to improve the metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handy: see the transformed dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7917             Please ,give me some help.Thanks before. \n",
       "25322    A positive development is that cooling has bee...\n",
       "22191    Witnesses say 200 to 500 people have so far di...\n",
       "18442    The 15-nation Economic Community of West Afric...\n",
       "1336     and in addition what time will it be possible ...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.        ,   5.83333333,   1.        ,  40.        ,\n",
       "          2.        ,   1.        ,   0.        ],\n",
       "       [ 19.        ,   4.63157895,   1.        , 106.        ,\n",
       "          6.        ,   4.        ,   1.        ],\n",
       "       [ 52.        ,   4.61538462,   1.        , 291.        ,\n",
       "         10.        ,   6.        ,   3.        ],\n",
       "       [ 30.        ,   5.33333333,   1.        , 189.        ,\n",
       "          5.        ,   4.        ,   3.        ],\n",
       "       [ 16.        ,   3.6875    ,   1.        ,  74.        ,\n",
       "          4.        ,   3.        ,   2.        ]])"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lr_v2.named_steps['features'].transform(x_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Improve model\n",
    "Here we will use grid search to find better parameters.\n",
    "\n",
    "### 7.1. Logistic Regression\n",
    "Here are the parameters that we can tune for the Logistic Regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'features', 'clf', 'features__n_jobs', 'features__transformer_list', 'features__transformer_weights', 'features__text', 'features__word-count', 'features__word-len', 'features__sentence-count', 'features__sentence-len', 'features__verb-count', 'features__text__analyzer', 'features__text__binary', 'features__text__decode_error', 'features__text__dtype', 'features__text__encoding', 'features__text__input', 'features__text__lowercase', 'features__text__max_df', 'features__text__max_features', 'features__text__min_df', 'features__text__ngram_range', 'features__text__norm', 'features__text__preprocessor', 'features__text__smooth_idf', 'features__text__stop_words', 'features__text__strip_accents', 'features__text__sublinear_tf', 'features__text__token_pattern', 'features__text__tokenizer', 'features__text__use_idf', 'features__text__vocabulary', 'clf__estimator__C', 'clf__estimator__class_weight', 'clf__estimator__dual', 'clf__estimator__fit_intercept', 'clf__estimator__intercept_scaling', 'clf__estimator__max_iter', 'clf__estimator__multi_class', 'clf__estimator__n_jobs', 'clf__estimator__penalty', 'clf__estimator__random_state', 'clf__estimator__solver', 'clf__estimator__tol', 'clf__estimator__verbose', 'clf__estimator__warm_start', 'clf__estimator', 'clf__n_jobs'])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_lr_v2.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only TF-IDF parameters (Logistic Regression has more or less only the C value to change)\n",
    "param_grid = {\n",
    "#     'features__text__ngram_range': ((1, 1), (1, 2)),\n",
    "#     'features__text__max_df': [0.5, 0.7, 1.0],\n",
    "#     'features__text__max_features': [3000, 5000],\n",
    "#     'features__text__norm': [None, 'l1', 'l2'],\n",
    "#     'features__text__binary': [True, False],\n",
    "    'features__text__use_idf': [True, False]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(pipeline_lr_v2, param_grid, cv=2, n_jobs=1, scoring=make_scorer(f1_score, average='micro'), verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__text__use_idf=True ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text__use_idf=True, score=0.5695081860495396, total= 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  5.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__text__use_idf=True ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text__use_idf=True, score=0.5710950729117004, total= 4.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 11.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] features__text__use_idf=False ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text__use_idf=False, score=0.5976489217053063, total= 4.0min\n",
      "[CV] features__text__use_idf=False ...................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  features__text__use_idf=False, score=0.5982751110723931, total= 4.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 21.8min finished\n",
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\svm\\base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('features', FeatureUnion(n_jobs=None,\n",
       "       transformer_list=[('text', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=5000, min_df=1,\n",
       "        ngr...ne, solver='liblinear',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "          n_jobs=None))]),\n",
       "       fit_params=None, iid='warn', n_jobs=1,\n",
       "       param_grid={'features__text__use_idf': [True, False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=make_scorer(f1_score, average=micro), verbose=3)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cv.fit(x_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5979620163888497"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__text__use_idf': False}"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: comment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only TF-IDF parameters (Logistic Regression has more or less only the C value to change)\n",
    "param_grid = {\n",
    "    'features__text__ngram_range': ((1, 1), (1, 2)),\n",
    "    'features__text__max_df': [0.5, 0.7, 1.0],\n",
    "    'features__text__max_features': [3000, 5000],\n",
    "    'features__text__norm': [None, 'l1', 'l2'],\n",
    "    'features__text__binary': [True, False],\n",
    "    'features__text__use_idf': [False]\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(pipeline_lr_v2, param_distributions=param_grid, cv=3, n_jobs=-1, n_iter=50, \n",
    "                        scoring=make_scorer(f1_score, average='micro'), verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 112 tasks      | elapsed: 92.0min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 121.3min finished\n",
      "C:\\Users\\David\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\externals\\joblib\\disk.py:122: UserWarning: Unable to delete folder C:\\Users\\David\\AppData\\Local\\Temp\\joblib_memmapping_folder_15668_6102637646 after 5 tentatives.\n",
      "  .format(folder_path, RM_SUBDIRS_N_RETRY))\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] Le processus ne peut pas accéder au fichier car ce fichier est utilisé par un autre processus: 'C:\\\\Users\\\\David\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_15668_6102637646\\\\15668-1830313074120-05c6707f38204e4da2ca0bd76745c3e8.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_terminate_backend\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;31m# in latter calls but we free as much memory as we can by deleting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[1;31m# the shared memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdelete_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\site-packages\\sklearn\\externals\\joblib\\disk.py\u001b[0m in \u001b[0;36mdelete_folder\u001b[1;34m(folder_path, onerror)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWindowsError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[1;31m# can't continue even if onerror hook returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    398\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\udadsnd-p5\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    399\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] Le processus ne peut pas accéder au fichier car ce fichier est utilisé par un autre processus: 'C:\\\\Users\\\\David\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_15668_6102637646\\\\15668-1830313074120-05c6707f38204e4da2ca0bd76745c3e8.pkl'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rs.fit(x_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-493-43cf89df942f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "rs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
